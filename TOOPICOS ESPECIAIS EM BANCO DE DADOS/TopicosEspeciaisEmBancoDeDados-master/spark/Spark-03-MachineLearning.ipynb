{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "denseVec = Vectors.dense(1.0, 2.0, 3.0)\n",
    "size = 3\n",
    "idx = [1, 2] # locations of non-zero elements in vector\n",
    "values = [2.0, 3.0]\n",
    "sparseVec = Vectors.sparse(size, idx, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "denseVec.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 2., 3.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparseVec.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|  red|good|    35|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|  red| bad|     2|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "|  red| bad|    16|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.json(\"data/simple-ml\")\n",
    "df.orderBy(\"value2\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fittedRF = supervised.fit(df)\n",
    "preparedDF = fittedRF.transform(df)\n",
    "preparedDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preparedDF.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+------+------------------+--------------------+-----+\n",
      "|color|lab|value1|            value2|            features|label|\n",
      "+-----+---+------+------------------+--------------------+-----+\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue|bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "+-----+---+------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\",featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.column.Column"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train['features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedLR = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rForm = RFormula()\n",
    "lr = LogisticRegression().setLabelCol(\"label\").setFeaturesCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "stages = [rForm, lr]\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "params = ParamGridBuilder()\\\n",
    "  .addGrid(rForm.formula, [\n",
    "    \"lab ~ . + color:value1\",\n",
    "    \"lab ~ . + color:value1 + color:value2\"])\\\n",
    "  .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "  .addGrid(lr.regParam, [0.1, 2.0])\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\\\n",
    "  .setMetricName(\"areaUnderROC\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setLabelCol(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "tvs = TrainValidationSplit()\\\n",
    "  .setTrainRatio(0.75)\\\n",
    "  .setEstimatorParamMaps(params)\\\n",
    "  .setEstimator(pipeline)\\\n",
    "  .setEvaluator(evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvsFitted = tvs.fit(train) # cria o modelo de Machine Lerning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333333"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(tvsFitted.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Engeneering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"data/retail-data/by-day/*.csv\")\\\n",
    "  .coalesce(5)\\\n",
    "  .where(\"Description IS NOT NULL\")\n",
    "fakeIntDF = spark.read.parquet(\"../data/simple-ml-integers\")\n",
    "simpleDF = spark.read.json(\"../data/simple-ml\")\n",
    "scaleDF = spark.read.parquet(\"../data/simple-ml-scaling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "540455"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|int1|int2|int3|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   7|   8|   9|\n",
      "|   4|   5|   6|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fakeIntDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaleDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|color| lab|value1|            value2|            features|label|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "|green|good|     1|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "| blue| bad|     8|14.386294994851129|(10,[2,3,6,9],[8....|  0.0|\n",
      "| blue| bad|    12|14.386294994851129|(10,[2,3,6,9],[12...|  0.0|\n",
      "|green|good|    15| 38.97187133755819|(10,[1,2,3,5,8],[...|  1.0|\n",
      "|green|good|    12|14.386294994851129|(10,[1,2,3,5,8],[...|  1.0|\n",
      "+-----+----+------+------------------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RFormula\n",
    "\n",
    "supervised = RFormula(formula=\"lab ~ . + color:value1 + color:value2\")\n",
    "supervised.fit(simpleDF).transform(simpleDF).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+\n",
      "|sum(Quantity)|count(1)|CustomerID|\n",
      "+-------------+--------+----------+\n",
      "|          119|      62|   14452.0|\n",
      "|          440|     143|   16916.0|\n",
      "|          630|      72|   17633.0|\n",
      "|           34|       6|   14768.0|\n",
      "|         1542|      30|   13094.0|\n",
      "+-------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "basicTransformation = SQLTransformer()\\\n",
    "  .setStatement(\"\"\"\n",
    "    SELECT sum(Quantity), count(*), CustomerID\n",
    "    FROM __THIS__\n",
    "    GROUP BY CustomerID\n",
    "  \"\"\")\n",
    "\n",
    "basicTransformation.transform(sales).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+--------------------------------------------+\n",
      "|int1|int2|int3|VectorAssembler_4807ac08488b444b2cc4__output|\n",
      "+----+----+----+--------------------------------------------+\n",
      "|   1|   2|   3|                               [1.0,2.0,3.0]|\n",
      "|   7|   8|   9|                               [7.0,8.0,9.0]|\n",
      "|   4|   5|   6|                               [4.0,5.0,6.0]|\n",
      "+----+----+----+--------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler().setInputCols([\"int1\", \"int2\", \"int3\"])\n",
    "va.transform(fakeIntDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|0.0|\n",
      "|1.0|\n",
      "|2.0|\n",
      "|3.0|\n",
      "|4.0|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "contDF = spark.range(20).selectExpr(\"cast(id as double)\")\n",
    "contDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------------------+\n",
      "|  id|Bucketizer_4e7fa983a35a09c51334__output|\n",
      "+----+---------------------------------------+\n",
      "| 0.0|                                    0.0|\n",
      "| 1.0|                                    0.0|\n",
      "| 2.0|                                    0.0|\n",
      "| 3.0|                                    0.0|\n",
      "| 4.0|                                    0.0|\n",
      "| 5.0|                                    1.0|\n",
      "| 6.0|                                    1.0|\n",
      "| 7.0|                                    1.0|\n",
      "| 8.0|                                    1.0|\n",
      "| 9.0|                                    1.0|\n",
      "|10.0|                                    2.0|\n",
      "|11.0|                                    2.0|\n",
      "|12.0|                                    2.0|\n",
      "|13.0|                                    2.0|\n",
      "|14.0|                                    2.0|\n",
      "|15.0|                                    2.0|\n",
      "|16.0|                                    2.0|\n",
      "|17.0|                                    2.0|\n",
      "|18.0|                                    2.0|\n",
      "|19.0|                                    2.0|\n",
      "+----+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "bucketBorders = [-1.0, 5.0, 10.0, 250.0, 600.0]\n",
    "bucketer = Bucketizer().setSplits(bucketBorders).setInputCol(\"id\")\n",
    "bucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------------------------------+\n",
      "|  id|QuantileDiscretizer_4d1fb96190b2a520bd4c__output|\n",
      "+----+------------------------------------------------+\n",
      "| 0.0|                                             0.0|\n",
      "| 1.0|                                             0.0|\n",
      "| 2.0|                                             0.0|\n",
      "| 3.0|                                             1.0|\n",
      "| 4.0|                                             1.0|\n",
      "| 5.0|                                             1.0|\n",
      "| 6.0|                                             1.0|\n",
      "| 7.0|                                             2.0|\n",
      "| 8.0|                                             2.0|\n",
      "| 9.0|                                             2.0|\n",
      "|10.0|                                             2.0|\n",
      "|11.0|                                             2.0|\n",
      "|12.0|                                             3.0|\n",
      "|13.0|                                             3.0|\n",
      "|14.0|                                             3.0|\n",
      "|15.0|                                             4.0|\n",
      "|16.0|                                             4.0|\n",
      "|17.0|                                             4.0|\n",
      "|18.0|                                             4.0|\n",
      "|19.0|                                             4.0|\n",
      "+----+------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "bucketer = QuantileDiscretizer().setNumBuckets(5).setInputCol(\"id\")\n",
    "fittedBucketer = bucketer.fit(contDF)\n",
    "fittedBucketer.transform(contDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaleDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-------------------------------------------+\n",
      "| id|      features|StandardScaler_4c3fbca61cb6ef3694b8__output|\n",
      "+---+--------------+-------------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                       [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|                       [2.39045721866878...|\n",
      "|  0|[1.0,0.1,-1.0]|                       [1.19522860933439...|\n",
      "|  1| [2.0,1.1,1.0]|                       [2.39045721866878...|\n",
      "|  1|[3.0,10.1,3.0]|                       [3.58568582800318...|\n",
      "+---+--------------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "sScaler = StandardScaler().setInputCol(\"features\")\n",
    "sScaler.fit(scaleDF).transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------+\n",
      "| id|      features|MinMaxScaler_44289355c35bb3be91d8__output|\n",
      "+---+--------------+-----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                            [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                            [7.5,5.5,7.5]|\n",
      "|  0|[1.0,0.1,-1.0]|                            [5.0,5.0,5.0]|\n",
      "|  1| [2.0,1.1,1.0]|                            [7.5,5.5,7.5]|\n",
      "|  1|[3.0,10.1,3.0]|                         [10.0,10.0,10.0]|\n",
      "+---+--------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MinMaxScaler\n",
    "minMax = MinMaxScaler().setMin(5).setMax(10).setInputCol(\"features\")\n",
    "fittedminMax = minMax.fit(scaleDF)\n",
    "fittedminMax.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------+\n",
      "| id|      features|MaxAbsScaler_4669bdbda04504bacac2__output|\n",
      "+---+--------------+-----------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                     [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|                     [0.66666666666666...|\n",
      "|  0|[1.0,0.1,-1.0]|                     [0.33333333333333...|\n",
      "|  1| [2.0,1.1,1.0]|                     [0.66666666666666...|\n",
      "|  1|[3.0,10.1,3.0]|                            [1.0,1.0,1.0]|\n",
      "+---+--------------+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import MaxAbsScaler\n",
    "maScaler = MaxAbsScaler().setInputCol(\"features\")\n",
    "fittedmaScaler = maScaler.fit(scaleDF)\n",
    "fittedmaScaler.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+-----------------------------------------------+\n",
      "| id|      features|ElementwiseProduct_485bb350325092783a70__output|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                               [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                               [20.0,16.5,20.0]|\n",
      "|  0|[1.0,0.1,-1.0]|                               [10.0,1.5,-20.0]|\n",
      "|  1| [2.0,1.1,1.0]|                               [20.0,16.5,20.0]|\n",
      "|  1|[3.0,10.1,3.0]|                              [30.0,151.5,60.0]|\n",
      "+---+--------------+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import ElementwiseProduct\n",
    "from pyspark.ml.linalg import Vectors\n",
    "scaleUpVec = Vectors.dense(10.0, 15.0, 20.0)\n",
    "scalingUp = ElementwiseProduct()\\\n",
    "  .setScalingVec(scaleUpVec)\\\n",
    "  .setInputCol(\"features\")\n",
    "scalingUp.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+---------------------------------------+\n",
      "| id|      features|Normalizer_42ef825fcb5f9158049b__output|\n",
      "+---+--------------+---------------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|                   [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|                   [0.48780487804878...|\n",
      "|  0|[1.0,0.1,-1.0]|                   [0.47619047619047...|\n",
      "|  1| [2.0,1.1,1.0]|                   [0.48780487804878...|\n",
      "|  1|[3.0,10.1,3.0]|                   [0.18633540372670...|\n",
      "+---+--------------+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Normalizer, StringIndexer\n",
    "manhattanDistance = Normalizer().setP(1).setInputCol(\"features\")\n",
    "manhattanDistance.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|valueInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "|  red| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    45| 38.97187133755819|     3.0|\n",
      "|green|good|     1|14.386294994851129|     2.0|\n",
      "| blue| bad|     8|14.386294994851129|     4.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     5.0|\n",
      "|green|good|    12|14.386294994851129|     0.0|\n",
      "|green| bad|    16|14.386294994851129|     1.0|\n",
      "|  red|good|    35|14.386294994851129|     6.0|\n",
      "|  red| bad|     1| 38.97187133755819|     2.0|\n",
      "|  red| bad|     2|14.386294994851129|     7.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "valIndexer = StringIndexer().setInputCol(\"value1\").setOutputCol(\"valueInd\")\n",
    "valIndexer.fit(simpleDF).transform(simpleDF).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+\n",
      "|color| lab|value1|            value2|labelInd|\n",
      "+-----+----+------+------------------+--------+\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|\n",
      "|green|good|     1|14.386294994851129|     1.0|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|\n",
      "|green|good|    15| 38.97187133755819|     1.0|\n",
      "|green|good|    12|14.386294994851129|     1.0|\n",
      "|green| bad|    16|14.386294994851129|     0.0|\n",
      "|  red|good|    35|14.386294994851129|     1.0|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|\n",
      "+-----+----+------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"lab\").setOutputCol(\"labelInd\")\n",
    "idxRes = lblIndxr.fit(simpleDF).transform(simpleDF)\n",
    "idxRes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+--------+---------+\n",
      "|color| lab|value1|            value2|labelInd|label_str|\n",
      "+-----+----+------+------------------+--------+---------+\n",
      "|green|good|     1|14.386294994851129|     1.0|     good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|      bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|      bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|     good|\n",
      "|green|good|    12|14.386294994851129|     1.0|     good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|      bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|     good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|      bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|      bad|\n",
      "|  red| bad|    16|14.386294994851129|     0.0|      bad|\n",
      "|  red|good|    45| 38.97187133755819|     1.0|     good|\n",
      "|green|good|     1|14.386294994851129|     1.0|     good|\n",
      "| blue| bad|     8|14.386294994851129|     0.0|      bad|\n",
      "| blue| bad|    12|14.386294994851129|     0.0|      bad|\n",
      "|green|good|    15| 38.97187133755819|     1.0|     good|\n",
      "|green|good|    12|14.386294994851129|     1.0|     good|\n",
      "|green| bad|    16|14.386294994851129|     0.0|      bad|\n",
      "|  red|good|    35|14.386294994851129|     1.0|     good|\n",
      "|  red| bad|     1| 38.97187133755819|     0.0|      bad|\n",
      "|  red| bad|     2|14.386294994851129|     0.0|      bad|\n",
      "+-----+----+------+------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "labelReverse = IndexToString().setInputCol(\"labelInd\").setOutputCol(\"label_str\")\n",
    "labelReverse.transform(idxRes).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|     features|label|\n",
      "+-------------+-----+\n",
      "|[1.0,2.0,3.0]|    1|\n",
      "|[2.0,5.0,6.0]|    2|\n",
      "|[1.0,8.0,9.0]|    3|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "idxIn = spark.createDataFrame([\n",
    "  (Vectors.dense(1, 2, 3),1),\n",
    "  (Vectors.dense(2, 5, 6),2),\n",
    "  (Vectors.dense(1, 8, 9),3)\n",
    "]).toDF(\"features\", \"label\")\n",
    "idxIn.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+-------------+\n",
      "|     features|label|        idxed|\n",
      "+-------------+-----+-------------+\n",
      "|[1.0,2.0,3.0]|    1|[0.0,2.0,3.0]|\n",
      "|[2.0,5.0,6.0]|    2|[1.0,5.0,6.0]|\n",
      "|[1.0,8.0,9.0]|    3|[0.0,8.0,9.0]|\n",
      "+-------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indxr = VectorIndexer()\\\n",
    "  .setInputCol(\"features\")\\\n",
    "  .setOutputCol(\"idxed\")\\\n",
    "  .setMaxCategories(2)\n",
    "indxr.fit(idxIn).transform(idxIn).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------------+\n",
      "|color| lab|value1|            value2|\n",
      "+-----+----+------+------------------+\n",
      "|green|good|     1|14.386294994851129|\n",
      "| blue| bad|     8|14.386294994851129|\n",
      "| blue| bad|    12|14.386294994851129|\n",
      "|green|good|    15| 38.97187133755819|\n",
      "|green|good|    12|14.386294994851129|\n",
      "+-----+----+------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simpleDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|color|colorInd|\n",
      "+-----+--------+\n",
      "|green|     1.0|\n",
      "| blue|     2.0|\n",
      "| blue|     2.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|green|     1.0|\n",
      "| blue|     2.0|\n",
      "| blue|     2.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|green|     1.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "|  red|     0.0|\n",
      "+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n",
      "+-----+--------+------------------------------------------+\n",
      "|color|colorInd|OneHotEncoder_46f382a1c2d71f995fa1__output|\n",
      "+-----+--------+------------------------------------------+\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "| blue|     2.0|                                 (2,[],[])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|green|     1.0|                             (2,[1],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "|  red|     0.0|                             (2,[0],[1.0])|\n",
      "+-----+--------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "lblIndxr = StringIndexer().setInputCol(\"color\").setOutputCol(\"colorInd\")\n",
    "colorLab = lblIndxr.fit(simpleDF).transform(simpleDF.select(\"color\"))\n",
    "print(colorLab.show())\n",
    "ohe = OneHotEncoder().setInputCol(\"colorInd\")\n",
    "print(ohe.transform(colorLab).show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.select(\"Description\"))\n",
    "tokenized.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+-------------------------------------+\n",
      "|Description                    |DescOut                              |\n",
      "+-------------------------------+-------------------------------------+\n",
      "|RABBIT NIGHT LIGHT             |[rabbit, night, light]               |\n",
      "|DOUGHNUT LIP GLOSS             |[doughnut, lip, gloss]               |\n",
      "|12 MESSAGE CARDS WITH ENVELOPES|[12, message, cards, with, envelopes]|\n",
      "|BLUE HARMONICA IN BOX          |[blue, harmonica, in, box]           |\n",
      "|GUMBALL COAT RACK              |[gumball, coat, rack]                |\n",
      "+-------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|         Description|           DescOut|\n",
      "+--------------------+------------------+\n",
      "|  RABBIT NIGHT LIGHT|            [ ,  ]|\n",
      "| DOUGHNUT LIP GLOSS |         [ ,  ,  ]|\n",
      "|12 MESSAGE CARDS ...|      [ ,  ,  ,  ]|\n",
      "|BLUE HARMONICA IN...|      [ ,  ,  ,  ]|\n",
      "|   GUMBALL COAT RACK|            [ ,  ]|\n",
      "|SKULLS  WATER TRA...|   [ ,  ,  ,  ,  ]|\n",
      "|FELTCRAFT GIRL AM...|         [ ,  ,  ]|\n",
      "|CAMOUFLAGE LED TORCH|            [ ,  ]|\n",
      "|WHITE SKULL HOT W...|   [ ,  ,  ,  ,  ]|\n",
      "|ENGLISH ROSE HOT ...|      [ ,  ,  ,  ]|\n",
      "|HOT WATER BOTTLE ...|      [ ,  ,  ,  ]|\n",
      "|SCOTTIE DOG HOT W...|      [ ,  ,  ,  ]|\n",
      "|ROSE CARAVAN DOOR...|            [ ,  ]|\n",
      "|GINGHAM HEART  DO...|      [ ,  ,  ,  ]|\n",
      "|STORAGE TIN VINTA...|         [ ,  ,  ]|\n",
      "|SET OF 4 KNICK KN...|[ ,  ,  ,  ,  ,  ]|\n",
      "|      POPCORN HOLDER|               [ ]|\n",
      "|GROW A FLYTRAP OR...|[ ,  ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTA...|   [ ,  ,  ,  ,  ]|\n",
      "|AIRLINE BAG VINTA...|   [ ,  ,  ,  ,  ]|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "rt = RegexTokenizer()\\\n",
    "  .setInputCol(\"Description\")\\\n",
    "  .setOutputCol(\"DescOut\")\\\n",
    "  .setPattern(\" \")\\\n",
    "  .setGaps(False)\\\n",
    "  .setToLowercase(True)\n",
    "rt.transform(sales.select(\"Description\")).show(20, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|             DescOut| Output_no_stopwords|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|[rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|[doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|[12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|[blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|[gumball, coat, r...|\n",
      "|SKULLS  WATER TRA...|[skulls, , water,...|[skulls, , water,...|\n",
      "|FELTCRAFT GIRL AM...|[feltcraft, girl,...|[feltcraft, girl,...|\n",
      "|CAMOUFLAGE LED TORCH|[camouflage, led,...|[camouflage, led,...|\n",
      "|WHITE SKULL HOT W...|[white, skull, ho...|[white, skull, ho...|\n",
      "|ENGLISH ROSE HOT ...|[english, rose, h...|[english, rose, h...|\n",
      "|HOT WATER BOTTLE ...|[hot, water, bott...|[hot, water, bott...|\n",
      "|SCOTTIE DOG HOT W...|[scottie, dog, ho...|[scottie, dog, ho...|\n",
      "|ROSE CARAVAN DOOR...|[rose, caravan, d...|[rose, caravan, d...|\n",
      "|GINGHAM HEART  DO...|[gingham, heart, ...|[gingham, heart, ...|\n",
      "|STORAGE TIN VINTA...|[storage, tin, vi...|[storage, tin, vi...|\n",
      "|SET OF 4 KNICK KN...|[set, of, 4, knic...|[set, 4, knick, k...|\n",
      "|      POPCORN HOLDER|   [popcorn, holder]|   [popcorn, holder]|\n",
      "|GROW A FLYTRAP OR...|[grow, a, flytrap...|[grow, flytrap, s...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|[airline, bag, vi...|\n",
      "|AIRLINE BAG VINTA...|[airline, bag, vi...|[airline, bag, vi...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "englishStopWords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stops = StopWordsRemover()\\\n",
    "  .setStopWords(englishStopWords)\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"Output_no_stopwords\")\n",
    "stops.transform(tokenized).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------+\n",
      "|DescOut                              |NGram_486f8411644dc52295d8__output   |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit, night, light]               |\n",
      "|[doughnut, lip, gloss]               |[doughnut, lip, gloss]               |\n",
      "|[12, message, cards, with, envelopes]|[12, message, cards, with, envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue, harmonica, in, box]           |\n",
      "|[gumball, coat, rack]                |[gumball, coat, rack]                |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls, , water, transfer, tattoos] |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft, girl, amelie, kit]       |\n",
      "|[camouflage, led, torch]             |[camouflage, led, torch]             |\n",
      "|[white, skull, hot, water, bottle]   |[white, skull, hot, water, bottle]   |\n",
      "|[english, rose, hot, water, bottle]  |[english, rose, hot, water, bottle]  |\n",
      "+-------------------------------------+-------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|DescOut                              |NGram_457ab7845eb18fefbfb5__output                     |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "|[rabbit, night, light]               |[rabbit night, night light]                            |\n",
      "|[doughnut, lip, gloss]               |[doughnut lip, lip gloss]                              |\n",
      "|[12, message, cards, with, envelopes]|[12 message, message cards, cards with, with envelopes]|\n",
      "|[blue, harmonica, in, box]           |[blue harmonica, harmonica in, in box]                 |\n",
      "|[gumball, coat, rack]                |[gumball coat, coat rack]                              |\n",
      "|[skulls, , water, transfer, tattoos] |[skulls ,  water, water transfer, transfer tattoos]    |\n",
      "|[feltcraft, girl, amelie, kit]       |[feltcraft girl, girl amelie, amelie kit]              |\n",
      "|[camouflage, led, torch]             |[camouflage led, led torch]                            |\n",
      "|[white, skull, hot, water, bottle]   |[white skull, skull hot, hot water, water bottle]      |\n",
      "|[english, rose, hot, water, bottle]  |[english rose, rose hot, hot water, water bottle]      |\n",
      "+-------------------------------------+-------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "unigram = NGram().setInputCol(\"DescOut\").setN(1)\n",
    "bigram = NGram().setInputCol(\"DescOut\").setN(2)\n",
    "unigram.transform(tokenized.select(\"DescOut\")).show(10,False)\n",
    "bigram.transform(tokenized.select(\"DescOut\")).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|             DescOut|            countVec|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|(500,[149,185,212...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|(500,[462,463,492...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|(500,[35,41,166],...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|(500,[10,16,36,35...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|(500,[228,280,408...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converts word into numerical representations\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"countVec\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(1)\\\n",
    "  .setMinDF(2)\n",
    "fittedCV = cv.fit(tokenized)\n",
    "dfcv = fittedCV.transform(tokenized)\n",
    "dfcv.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfcv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       Description|\n",
      "+------------------+\n",
      "|RABBIT NIGHT LIGHT|\n",
      "+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcv.select('Description').show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "Row(countVec=SparseVector(500, {149: 1.0, 185: 1.0, 212: 1.0}))\n",
      "500\n",
      "[149 185 212]\n",
      "[1. 1. 1.]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "col = dfcv.select('countVec').take(1)\n",
    "print(type(col))\n",
    "print(col[0])\n",
    "print((col[0][0]).size) # size of sparse vector\n",
    "print((col[0][0]).indices) # dict indices\n",
    "print((col[0][0]).values)\n",
    "print(col[0][0][212])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|         Description|             DescOut|\n",
      "+--------------------+--------------------+\n",
      "|  RABBIT NIGHT LIGHT|[rabbit, night, l...|\n",
      "| DOUGHNUT LIP GLOSS |[doughnut, lip, g...|\n",
      "|12 MESSAGE CARDS ...|[12, message, car...|\n",
      "|BLUE HARMONICA IN...|[blue, harmonica,...|\n",
      "|   GUMBALL COAT RACK|[gumball, coat, r...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+\n",
      "|DescOut                                |\n",
      "+---------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[red, floral, feltcraft, shoulder, bag]|\n",
      "|[alarm, clock, bakelike, red]          |\n",
      "|[pin, cushion, babushka, red]          |\n",
      "|[red, retrospot, mini, cases]          |\n",
      "|[red, kitchen, scales]                 |\n",
      "|[gingham, heart, , doorstop, red]      |\n",
      "|[large, red, babushka, notebook]       |\n",
      "|[red, retrospot, oven, glove]          |\n",
      "|[red, retrospot, plate]                |\n",
      "+---------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfIdfIn = tokenized\\\n",
    "  .where(\"array_contains(DescOut, 'red')\")\\\n",
    "  .select(\"DescOut\")\\\n",
    "  .limit(10)\n",
    "tfIdfIn.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "tf = HashingTF()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"TFOut\")\\\n",
    "  .setNumFeatures(10000)\n",
    "idf = IDF()\\\n",
    "  .setInputCol(\"TFOut\")\\\n",
    "  .setOutputCol(\"IDFOut\")\\\n",
    "  .setMinDocFreq(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|DescOut                                |TFOut                                                   |IDFOut                                                                                                              |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[red, floral, feltcraft, shoulder, bag]|(10000,[155,1152,4291,5981,6756],[1.0,1.0,1.0,1.0,1.0]) |(10000,[155,1152,4291,5981,6756],[0.0,0.0,0.0,0.0,0.0])                                                             |\n",
      "|[alarm, clock, bakelike, red]          |(10000,[4291,4852,4995,9668],[1.0,1.0,1.0,1.0])         |(10000,[4291,4852,4995,9668],[0.0,0.0,0.0,0.0])                                                                     |\n",
      "|[pin, cushion, babushka, red]          |(10000,[4291,5111,5673,7153],[1.0,1.0,1.0,1.0])         |(10000,[4291,5111,5673,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, mini, cases]          |(10000,[547,1576,2591,4291],[1.0,1.0,1.0,1.0])          |(10000,[547,1576,2591,4291],[0.0,0.0,1.0116009116784799,0.0])                                                       |\n",
      "|[red, kitchen, scales]                 |(10000,[3461,4291,6214],[1.0,1.0,1.0])                  |(10000,[3461,4291,6214],[0.0,0.0,0.0])                                                                              |\n",
      "|[gingham, heart, , doorstop, red]      |(10000,[3372,4291,4370,6594,9160],[1.0,1.0,1.0,1.0,1.0])|(10000,[3372,4291,4370,6594,9160],[1.2992829841302609,0.0,1.2992829841302609,1.2992829841302609,1.2992829841302609])|\n",
      "|[large, red, babushka, notebook]       |(10000,[2782,2787,4291,7153],[1.0,1.0,1.0,1.0])         |(10000,[2782,2787,4291,7153],[0.0,0.0,0.0,1.2992829841302609])                                                      |\n",
      "|[red, retrospot, oven, glove]          |(10000,[302,2591,4291,8242],[1.0,1.0,1.0,1.0])          |(10000,[302,2591,4291,8242],[0.0,1.0116009116784799,0.0,0.0])                                                       |\n",
      "|[red, retrospot, plate]                |(10000,[2591,4291,4456],[1.0,1.0,1.0])                  |(10000,[2591,4291,4456],[1.0116009116784799,0.0,0.0])                                                               |\n",
      "+---------------------------------------+--------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idf.fit(tf.transform(tfIdfIn)).transform(tf.transform(tfIdfIn)).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: [Hi, I, heard, about, Spark] => \n",
      "Vector: [0.012779767811298371,-0.09340975657105446,-0.10830843970179559]\n",
      "\n",
      "Text: [I, wish, Java, could, use, case, classes] => \n",
      "Vector: [0.07612769335641392,0.03451743721961975,-0.04290600613291774]\n",
      "\n",
      "Text: [Logistic, regression, models, are, neat] => \n",
      "Vector: [-0.06759414225816728,0.045298346877098085,0.05302179120481015]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "# Input data: Each row is a bag of words from a sentence or document.\n",
    "documentDF = spark.createDataFrame([\n",
    "    (\"Hi I heard about Spark\".split(\" \"), ),\n",
    "    (\"I wish Java could use case classes\".split(\" \"), ),\n",
    "    (\"Logistic regression models are neat\".split(\" \"), )\n",
    "], [\"text\"])\n",
    "# Learn a mapping from words to Vectors.\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\",\n",
    "  outputCol=\"result\")\n",
    "model = word2Vec.fit(documentDF)\n",
    "result = model.transform(documentDF)\n",
    "for row in result.collect():\n",
    "    text, vector = row\n",
    "    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Manipulation\n",
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      features|\n",
      "+---+--------------+\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  0|[1.0,0.1,-1.0]|\n",
      "|  1| [2.0,1.1,1.0]|\n",
      "|  1|[3.0,10.1,3.0]|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scaleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+------------------------------------------+\n",
      "|id |features      |PCA_4c96952c0cfacdb535a4__output          |\n",
      "+---+--------------+------------------------------------------+\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|0  |[1.0,0.1,-1.0]|[0.07137194992484153,-0.45266548881478463]|\n",
      "|1  |[2.0,1.1,1.0] |[-1.6804946984073725,1.2593401322219144]  |\n",
      "|1  |[3.0,10.1,3.0]|[-10.872398139848944,0.030962697060149758]|\n",
      "+---+--------------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA().setInputCol(\"features\").setK(2)\n",
    "pca.fit(scaleDF).transform(scaleDF).show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "fittedPCA = pca.fit(scaleDF)\n",
    "fittedPCA.write().overwrite().save(\"fittedPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+--------------------------------+\n",
      "| id|      features|PCA_4c96952c0cfacdb535a4__output|\n",
      "+---+--------------+--------------------------------+\n",
      "|  0|[1.0,0.1,-1.0]|            [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|            [-1.6804946984073...|\n",
      "|  0|[1.0,0.1,-1.0]|            [0.07137194992484...|\n",
      "|  1| [2.0,1.1,1.0]|            [-1.6804946984073...|\n",
      "|  1|[3.0,10.1,3.0]|            [-10.872398139848...|\n",
      "+---+--------------+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCAModel\n",
    "loadedPCA = PCAModel.load(\"fittedPCA\")\n",
    "loadedPCA.transform(scaleDF).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"Python\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[3.0,10.1,3.0]|  1.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bInput = spark.read.format(\"parquet\").load(\"data/binary-classification\")\\\n",
    "  .selectExpr(\"features\", \"cast(label as double) as label\")\n",
    "bInput.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "#print (lr.explainParams() ) # see all parameters\n",
    "lrModel = lr.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.848741326854998,0.3535658901019746,14.814900276915903]\n",
      "-10.225695864481022\n"
     ]
    }
   ],
   "source": [
    "print (lrModel.coefficients)\n",
    "print (lrModel.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "+---+------------------+\n",
      "|FPR|               TPR|\n",
      "+---+------------------+\n",
      "|0.0|               0.0|\n",
      "|0.0|0.3333333333333333|\n",
      "|0.0|               1.0|\n",
      "|1.0|               1.0|\n",
      "|1.0|               1.0|\n",
      "+---+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "summary = lrModel.summary\n",
    "print (summary.areaUnderROC)\n",
    "print(summary.roc.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|            recall|precision|\n",
      "+------------------+---------+\n",
      "|               0.0|      1.0|\n",
      "|0.3333333333333333|      1.0|\n",
      "|               1.0|      1.0|\n",
      "|               1.0|      0.6|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.pr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6730116670092565,\n",
       " 0.5042829330409727,\n",
       " 0.36356862066874396,\n",
       " 0.1252407018038337,\n",
       " 0.08532556611276214,\n",
       " 0.03550487641573045,\n",
       " 0.01819649450857126,\n",
       " 0.008817369922959133,\n",
       " 0.004413673785392145,\n",
       " 0.002194038351234711,\n",
       " 0.0010965641148080857,\n",
       " 0.000547657551985314,\n",
       " 0.00027376237951490083,\n",
       " 0.00013684652236574735,\n",
       " 6.841809037070565e-05,\n",
       " 3.420707791038474e-05,\n",
       " 1.710317666423187e-05,\n",
       " 8.551470106426846e-06,\n",
       " 4.275703677941403e-06,\n",
       " 2.1378240117781205e-06,\n",
       " 1.0688564054651793e-06,\n",
       " 5.342600202575221e-07,\n",
       " 2.6681351058971897e-07,\n",
       " 1.32046278653146e-07,\n",
       " 6.768401481683304e-08,\n",
       " 3.3145477184846346e-08,\n",
       " 1.615143883749056e-08,\n",
       " 8.309350118268702e-09]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.objectiveHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "#print (dt.explainParams())\n",
    "dtModel = dt.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassificationModel"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rfClassifier = RandomForestClassifier()\n",
    "#print (rfClassifier.explainParams())\n",
    "trainedModel = rfClassifier.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbtClassifier = GBTClassifier()\n",
    "#print (gbtClassifier.explainParams())\n",
    "trainedModel = gbtClassifier.fit(bInput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "nb = NaiveBayes()\n",
    "#print (nb.explainParams() )\n",
    "trainedModel = nb.fit(bInput.where(\"label != 0\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[3.0,10.1,3.0]|  1.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bInput.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0), (0.0, 0.0), (0.0, 0.0), (1.0, 1.0), (1.0, 1.0)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluator\n",
    "model = dtModel # decision tree model\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "out = model.transform(bInput)\\\n",
    "  .select(\"prediction\", \"label\")\\\n",
    "  .rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "out.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = BinaryClassificationMetrics(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print (metrics.areaUnderPR)\n",
    "print (metrics.areaUnderROC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[3.0,10.1,3.0]|  2.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "| [2.0,4.1,1.0]|  2.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.load(\"data/regression\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "#print (lr.explainParams())\n",
    "lrModel = lr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.1280504658561019|\n",
      "|-0.14468269261572098|\n",
      "| -0.4190383262242056|\n",
      "| -0.4190383262242056|\n",
      "|  0.8547088792080308|\n",
      "+--------------------+\n",
      "\n",
      "6\n",
      "[0.5000000000000001, 0.4315295810362787, 0.3132335933881021, 0.312256926665541, 0.3091506081983029, 0.3091505893348027]\n",
      "0.47308424392175985\n",
      "0.720239122691221\n"
     ]
    }
   ],
   "source": [
    "summary = lrModel.summary\n",
    "summary.residuals.show()\n",
    "print (summary.totalIterations)\n",
    "print (summary.objectiveHistory)\n",
    "print (summary.rootMeanSquaredError)\n",
    "print (summary.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Param(parent='LinearRegression_4bab927054396d9080fd', name='predictionCol', doc='prediction column name')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrModel.predictionCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "glr = GeneralizedLinearRegression()\\\n",
    "  .setFamily(\"gaussian\")\\\n",
    "  .setLink(\"identity\")\\\n",
    "  .setMaxIter(10)\\\n",
    "  .setRegParam(0.3)\\\n",
    "  .setLinkPredictionCol(\"linkOut\")\n",
    "#print (glr.explainParams())\n",
    "glrModel = glr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "#print (dtr.explainParams())\n",
    "dtrModel = dtr.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "rf =  RandomForestRegressor()\n",
    "#print rf.explainParams()\n",
    "rfModel = rf.fit(df)\n",
    "gbt = GBTRegressor()\n",
    "#print gbt.explainParams()\n",
    "gbtModel = gbt.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|      features|label|\n",
      "+--------------+-----+\n",
      "|[3.0,10.1,3.0]|  2.0|\n",
      "| [2.0,1.1,1.0]|  1.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "|[1.0,0.1,-1.0]|  0.0|\n",
      "| [2.0,4.1,1.0]|  2.0|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- features: vector (nullable = true)\n",
      " |-- label: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import GeneralizedLinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "glr = GeneralizedLinearRegression().setFamily(\"gaussian\").setLink(\"identity\")\n",
    "pipeline = Pipeline().setStages([glr])\n",
    "params = ParamGridBuilder().addGrid(glr.regParam, [0.0, 0.5, 1.0]).build()\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setMetricName(\"rmse\")\\\n",
    "  .setPredictionCol(\"prediction\")\\\n",
    "  .setLabelCol(\"label\")\n",
    "cv = CrossValidator()\\\n",
    "  .setEstimator(pipeline)\\\n",
    "  .setEvaluator(evaluator)\\\n",
    "  .setEstimatorParamMaps(params)\\\n",
    "  .setNumFolds(2) # should always be 3 or more but this dataset is small\n",
    "model = cv.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.15705521472392636\n",
      "RMSE: 0.39630192369445594\n",
      "R-squared: 0.803680981595092\n",
      "MAE: 0.31411042944785267\n",
      "Explained variance: 0.6429447852760728\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "model = model\n",
    "out = model.transform(df)\\\n",
    "  .select(\"prediction\", \"label\").rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = RegressionMetrics(out)\n",
    "print ( \"MSE: \" + str(metrics.meanSquaredError))\n",
    "print ( \"RMSE: \" + str(metrics.rootMeanSquaredError))\n",
    "print ( \"R-squared: \" + str(metrics.r2))\n",
    "print ( \"MAE: \" + str(metrics.meanAbsoluteError) )\n",
    "print ( \"Explained variance: \" + str(metrics.explainedVariance) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "ratings = spark.read.text(\"data/sample_movielens_ratings.txt\")\\\n",
    "  .rdd.toDF()\\\n",
    "  .selectExpr(\"split(value , '::') as col\")\\\n",
    "  .selectExpr(\n",
    "    \"cast(col[0] as int) as userId\",\n",
    "    \"cast(col[1] as int) as movieId\",\n",
    "    \"cast(col[2] as float) as rating\",\n",
    "    \"cast(col[3] as long) as timestamp\")\n",
    "training, test = ratings.randomSplit([0.8, 0.2])\n",
    "als = ALS()\\\n",
    "  .setMaxIter(5)\\\n",
    "  .setRegParam(0.01)\\\n",
    "  .setUserCol(\"userId\")\\\n",
    "  .setItemCol(\"movieId\")\\\n",
    "  .setRatingCol(\"rating\")\n",
    "#print (als.explainParams())\n",
    "alsModel = als.fit(training)\n",
    "predictions = alsModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=0, movieId=2, rating=3.0, timestamp=1424380312),\n",
       " Row(userId=0, movieId=3, rating=1.0, timestamp=1424380312),\n",
       " Row(userId=0, movieId=5, rating=2.0, timestamp=1424380312),\n",
       " Row(userId=0, movieId=9, rating=4.0, timestamp=1424380312),\n",
       " Row(userId=0, movieId=11, rating=1.0, timestamp=1424380312)]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=27, movieId=31, rating=1.0, timestamp=1424380312, prediction=1.0664252042770386),\n",
       " Row(userId=5, movieId=31, rating=1.0, timestamp=1424380312, prediction=-0.39794427156448364),\n",
       " Row(userId=29, movieId=31, rating=1.0, timestamp=1424380312, prediction=1.0999280214309692),\n",
       " Row(userId=5, movieId=85, rating=1.0, timestamp=1424380312, prediction=0.4406394064426422),\n",
       " Row(userId=8, movieId=85, rating=5.0, timestamp=1424380312, prediction=2.702888250350952)]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(predictions)\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+\n",
      "|userId|            col|\n",
      "+------+---------------+\n",
      "|    28|[92, 5.0727963]|\n",
      "|    28|[81, 4.8877063]|\n",
      "|    28|[12, 4.8776913]|\n",
      "|    28|[89, 4.0290647]|\n",
      "|    28| [2, 3.9999743]|\n",
      "|    28| [49, 3.962494]|\n",
      "|    28|[53, 3.9392726]|\n",
      "|    28|[82, 3.7512054]|\n",
      "|    28|[96, 3.4206526]|\n",
      "|    28|[46, 3.3429942]|\n",
      "|    26|[19, 7.0987396]|\n",
      "|    26|[47, 5.6084614]|\n",
      "|    26|[34, 5.5124083]|\n",
      "|    26|[39, 5.2580194]|\n",
      "|    26| [24, 5.198512]|\n",
      "|    26|[12, 5.0430365]|\n",
      "|    26|  [9, 4.975503]|\n",
      "|    26|  [7, 4.956293]|\n",
      "|    26| [23, 4.932685]|\n",
      "|    26|   [88, 4.8356]|\n",
      "+------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+---------------+\n",
      "|movieId|            col|\n",
      "+-------+---------------+\n",
      "|     31|[21, 4.1652923]|\n",
      "|     31| [12, 3.802142]|\n",
      "|     31| [14, 3.046466]|\n",
      "|     31| [9, 2.8910606]|\n",
      "|     31|[10, 2.8666377]|\n",
      "|     31|  [8, 2.765773]|\n",
      "|     31| [7, 2.7095041]|\n",
      "|     31|[15, 2.6663263]|\n",
      "|     31| [3, 2.4565802]|\n",
      "|     31|[25, 2.3014078]|\n",
      "|     85|[16, 5.1440945]|\n",
      "|     85| [7, 3.5886767]|\n",
      "|     85| [6, 3.1831617]|\n",
      "|     85| [3, 2.9103796]|\n",
      "|     85|[21, 2.8675344]|\n",
      "|     85| [8, 2.7028883]|\n",
      "|     85|[10, 2.5184717]|\n",
      "|     85|[14, 2.4846668]|\n",
      "|     85|[20, 2.1167123]|\n",
      "|     85| [0, 2.0895686]|\n",
      "+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alsModel.recommendForAllUsers(10)\\\n",
    "  .selectExpr(\"userId\", \"explode(recommendations)\").show()\n",
    "alsModel.recommendForAllItems(10)\\\n",
    "  .selectExpr(\"movieId\", \"explode(recommendations)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = -0.860416\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\\\n",
    "  .setMetricName(\"r2\")\\\n",
    "  .setLabelCol(\"rating\")\\\n",
    "  .setPredictionCol(\"prediction\")\n",
    "r2 = evaluator.evaluate(predictions)\n",
    "print(\"R2 = %f\" % r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 1.6025909846682076\n",
      "R-squared = -0.30107443747006335\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "regComparison = predictions.select(\"rating\", \"prediction\")\\\n",
    "  .rdd.map(lambda x: (float(x[0]), float(x[1])))\n",
    "metrics = RegressionMetrics(regComparison)\n",
    "print(\"RMSE = %s\" % metrics.rootMeanSquaredError)\n",
    "# R-squared\n",
    "print(\"R-squared = %s\" % metrics.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import RankingMetrics, RegressionMetrics\n",
    "from pyspark.sql.functions import col, expr\n",
    "perUserActual = predictions\\\n",
    "  .where(\"rating > 2.5\")\\\n",
    "  .groupBy(\"userId\")\\\n",
    "  .agg(expr(\"collect_set(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "perUserPredictions = predictions\\\n",
    "  .orderBy(col(\"userId\"), expr(\"prediction DESC\"))\\\n",
    "  .groupBy(\"userId\")\\\n",
    "  .agg(expr(\"collect_list(movieId) as movies\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "perUserActualvPred = perUserActual.join(perUserPredictions, [\"userId\"]).rdd\\\n",
    "  .map(lambda row: (row[1], row[2][:15]))\n",
    "ranks = RankingMetrics(perUserActualvPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22926870138408603"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.meanAveragePrecision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.476923076923077"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks.precisionAt(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "va = VectorAssembler()\\\n",
    "  .setInputCols([\"Quantity\", \"UnitPrice\"])\\\n",
    "  .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|   features|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|[48.0,1.79]|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|[20.0,1.25]|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|[24.0,1.65]|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|[24.0,1.25]|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom| [6.0,2.55]|\n",
      "|   580538|    21544|SKULLS  WATER TRA...|      48|2011-12-05 08:38:00|     0.85|   14075.0|United Kingdom|[48.0,0.85]|\n",
      "|   580538|    23126|FELTCRAFT GIRL AM...|       8|2011-12-05 08:38:00|     4.95|   14075.0|United Kingdom| [8.0,4.95]|\n",
      "|   580538|    21833|CAMOUFLAGE LED TORCH|      24|2011-12-05 08:38:00|     1.69|   14075.0|United Kingdom|[24.0,1.69]|\n",
      "|   580539|    21479|WHITE SKULL HOT W...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom| [4.0,4.25]|\n",
      "|   580539|   84030E|ENGLISH ROSE HOT ...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom| [4.0,4.25]|\n",
      "|   580539|    23355|HOT WATER BOTTLE ...|       4|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom| [4.0,4.95]|\n",
      "|   580539|    22111|SCOTTIE DOG HOT W...|       3|2011-12-05 08:39:00|     4.95|   18180.0|United Kingdom| [3.0,4.95]|\n",
      "|   580539|    21115|ROSE CARAVAN DOOR...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom| [8.0,1.95]|\n",
      "|   580539|    21411|GINGHAM HEART  DO...|       8|2011-12-05 08:39:00|     1.95|   18180.0|United Kingdom| [8.0,1.95]|\n",
      "|   580539|    23235|STORAGE TIN VINTA...|      12|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|[12.0,1.25]|\n",
      "|   580539|    23239|SET OF 4 KNICK KN...|       6|2011-12-05 08:39:00|     1.65|   18180.0|United Kingdom| [6.0,1.65]|\n",
      "|   580539|    22197|      POPCORN HOLDER|      36|2011-12-05 08:39:00|     0.85|   18180.0|United Kingdom|[36.0,0.85]|\n",
      "|   580539|    22693|GROW A FLYTRAP OR...|      24|2011-12-05 08:39:00|     1.25|   18180.0|United Kingdom|[24.0,1.25]|\n",
      "|   580539|    22372|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom| [4.0,4.25]|\n",
      "|   580539|    22375|AIRLINE BAG VINTA...|       4|2011-12-05 08:39:00|     4.25|   18180.0|United Kingdom| [4.0,4.25]|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales = va.transform(spark.read.format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"../data/retail-data/by-day/*.csv\")\n",
    "  .limit(50)\n",
    "  .coalesce(1)\n",
    "  .where(\"Description IS NOT NULL\"))\n",
    "\n",
    "sales.cache().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "km = KMeans().setK(5)\n",
    "#print (km.explainParams())\n",
    "kmModel = km.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 8, 29, 2, 1]\n",
      "Cluster Centers: \n",
      "[23.2    0.956]\n",
      "[ 2.5     11.24375]\n",
      "[7.55172414 2.77172414]\n",
      "[48.    1.32]\n",
      "[36.    0.85]\n"
     ]
    }
   ],
   "source": [
    "summary = kmModel.summary\n",
    "print (summary.clusterSizes) # number of points\n",
    "kmModel.computeCost(sales)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import BisectingKMeans\n",
    "bkm = BisectingKMeans().setK(5).setMaxIter(5)\n",
    "bkmModel = bkm.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16, 8, 13, 10, 3]\n",
      "Cluster Centers: \n",
      "[23.2    0.956]\n",
      "[ 2.5     11.24375]\n",
      "[7.55172414 2.77172414]\n",
      "[48.    1.32]\n",
      "[36.    0.85]\n"
     ]
    }
   ],
   "source": [
    "summary = bkmModel.summary\n",
    "print (summary.clusterSizes) # number of points\n",
    "kmModel.computeCost(sales)\n",
    "centers = kmModel.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "gmm = GaussianMixture().setK(5)\n",
    "#print (gmm.explainParams())\n",
    "model = gmm.fit(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16503937777770641, 0.35496420094056985, 0.06003637101912308, 0.1999636297743671, 0.21999642048823354]\n",
      "+--------------------+--------------------+\n",
      "|                mean|                 cov|\n",
      "+--------------------+--------------------+\n",
      "|[2.54180583818530...|0.785769315153778...|\n",
      "|[5.07243095740621...|2.059950971034034...|\n",
      "|[43.9877864408847...|32.22707068867282...|\n",
      "|[23.1998836372414...|2.560279258630084...|\n",
      "|[11.6364190345020...|1.322132750446848...|\n",
      "+--------------------+--------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "summary = model.summary\n",
    "print (model.weights)\n",
    "print(model.gaussiansDF.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         3|\n",
      "|         3|\n",
      "|         3|\n",
      "|         1|\n",
      "|         2|\n",
      "|         4|\n",
      "|         3|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         1|\n",
      "|         4|\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         1|\n",
      "|         1|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.cluster.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 18, 3, 10, 11]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.clusterSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         probability|\n",
      "+--------------------+\n",
      "|[1.37632400885157...|\n",
      "|[4.89041912245635...|\n",
      "|[1.67299627008735...|\n",
      "|[7.43321003719004...|\n",
      "|[1.46369160111044...|\n",
      "|[1.37558526306392...|\n",
      "|[1.60356443149152...|\n",
      "|[1.88134064420900...|\n",
      "|[0.00607298994290...|\n",
      "|[0.00607298994290...|\n",
      "|[0.01533941819191...|\n",
      "|[0.03609689525825...|\n",
      "|[2.32052388159997...|\n",
      "|[2.32052388159997...|\n",
      "|[4.95219998798691...|\n",
      "|[3.92383092236182...|\n",
      "|[1.36205103659923...|\n",
      "|[7.43321003719004...|\n",
      "|[0.00607298994290...|\n",
      "|[0.00607298994290...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary.probability.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA - Topic Modeling topic modelling on text documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|   features|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "|   580538|    23084|  RABBIT NIGHT LIGHT|      48|2011-12-05 08:38:00|     1.79|   14075.0|United Kingdom|[48.0,1.79]|\n",
      "|   580538|    23077| DOUGHNUT LIP GLOSS |      20|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|[20.0,1.25]|\n",
      "|   580538|    22906|12 MESSAGE CARDS ...|      24|2011-12-05 08:38:00|     1.65|   14075.0|United Kingdom|[24.0,1.65]|\n",
      "|   580538|    21914|BLUE HARMONICA IN...|      24|2011-12-05 08:38:00|     1.25|   14075.0|United Kingdom|[24.0,1.25]|\n",
      "|   580538|    22467|   GUMBALL COAT RACK|       6|2011-12-05 08:38:00|     2.55|   14075.0|United Kingdom| [6.0,2.55]|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "tkn = Tokenizer().setInputCol(\"Description\").setOutputCol(\"DescOut\")\n",
    "tokenized = tkn.transform(sales.drop(\"features\"))\n",
    "cv = CountVectorizer()\\\n",
    "  .setInputCol(\"DescOut\")\\\n",
    "  .setOutputCol(\"features\")\\\n",
    "  .setVocabSize(500)\\\n",
    "  .setMinTF(0)\\\n",
    "  .setMinDF(0)\\\n",
    "  .setBinary(True)\n",
    "cvFitted = cv.fit(tokenized)\n",
    "prepped = cvFitted.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "lda = LDA().setK(10).setMaxIter(5)\n",
    "#print (lda.explainParams())\n",
    "model = lda.fit(prepped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+\n",
      "|topic|         termIndices|         termWeights|\n",
      "+-----+--------------------+--------------------+\n",
      "|    0|[137, 90, 49, 11,...|[0.00891024825887...|\n",
      "|    1|[56, 29, 98, 16, 14]|[0.00916648435544...|\n",
      "|    2|[15, 131, 45, 129...|[0.00897001776729...|\n",
      "|    3|[6, 125, 78, 65, 27]|[0.00902238740730...|\n",
      "|    4|[103, 55, 62, 108...|[0.00933169757372...|\n",
      "|    5|[11, 23, 13, 5, 100]|[0.01462723703112...|\n",
      "|    6|  [2, 7, 16, 94, 79]|[0.01779008733823...|\n",
      "|    7|[30, 28, 73, 90, 10]|[0.01214763272764...|\n",
      "|    8|   [0, 3, 1, 14, 35]|[0.01490962515356...|\n",
      "|    9|[120, 94, 78, 131...|[0.00910675353890...|\n",
      "+-----+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.describeTopics(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['water',\n",
       " 'hot',\n",
       " 'vintage',\n",
       " 'bottle',\n",
       " 'paperweight',\n",
       " '6',\n",
       " 'home',\n",
       " 'doormat',\n",
       " 'landmark',\n",
       " 'bicycle']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvFitted.vocabulary[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
